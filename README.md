# ğŸ‘¶ğŸ»  Comparing the Effect of Transfer Learning on ResNet for Classification-Problems
<br/>
  
### 1. &nbsp; Research Objective <br/><br/>

- _In this study, I train a ResNet50 model using the AI-Hub Korean Face Image dataset to estimate the age of individuals.  In the process, I compare the classification performance of the Original ResNet50 model and the TL-ResNet50 model that applies transfer learning (TL) technique. The main hypothesis of this research is as follows:_  <br/>

  - _"Under the condition of training with a small dataset and a small number of epochs, the model using transfer learning will perform better."_ <br/><br/>

- _This hypothesis is based on the following, referencing the ResNet-50 model structure from the paper â€œDeep Residual Learning for Image Recognition" (CVPR, 2016) and the transfer learning and fine-tuning experiments from the paper "Best Practices for Fine-tuning Visual Classifiers to New Domains" (ECCV, 2016)._  <br/>

  - _Model complexity and number of epochs: Overfitting may occur when training complex models with many layers and parameters (e.g., ResNet-50, VGG-19), which can be avoided by training with a small number of epochs. However, training with a small number of epochs may cause another problem: underfitting due to insufficient training on the entire dataset._ <br/>
  
  - _Data quantity: Training a model with a small dataset can lead to overfitting due to lack of diversity in the data. This can be addressed through transfer learning and fine-tuning, which leverages the generalization ability of pre-trained model trained with large datasets, and generally performs well even with a small amount of data (diversity) because it performs new tasks (model training) based on the weights of pre-trained model that have trained patterns from a variety of images._ <br/><br/>
    
- _The TL-ResNet50 model, which estimates the age of an individual based on the above hypothesis, is expected to be applied to various fields, such as predicting the age of a criminal._ <br/><br/><br/> 

### 2. &nbsp; Key Components of the Neural Network Model and Experimental Settings  <br/><br/>

- _Convolutional Layer_<br/>

  - _Number of filters : (32 or 64)  /  Kernel size: (3 x 3)._ <br/>
  
  - _The convolutional layer extracts local features using filters (kernels) and generates feature maps._ <br/><br/>

- _Pooling Layer_<br/>

  - _Pooling size : (2, 2)._<br/>
  
  - _The pooling layer provides spatial invariance, reduces the size of feature maps to decrease computational complexity, and emphasizes abstracted features._ <br/><br/>

- _Dense Layer_ <br/>

  - _Number of nodes: (512 or 10)._ <br/>

  - _The dense layer is a traditional neural network layer that connects all inputs and outputs. It learns abstract features and outputs probability distribution for various classes._<br/><br/>
  
- _Dropout Layer_ <br/>

  - _The dropout layer is one of the regularization techniques used to reduce overfitting during the neural network training process._ <br/>

  - _Dropout randomly deactivates some units (neurons) of the neural network during training, preventing the model from relying too heavily on specific units and improving generalization capability._<br/><br/>

- _Activation function for hidden layers : ReLU Function_ <br/>

  - _The ReLU function is a non-linear function that outputs 0 for negative input values and keeps the output as is for positive input values._ <br/>

  - _To alleviate the issue of gradient vanishing caused by weight initialization when using ReLU activation function, the weights of the hidden layers were initialized using He initialization._<br/><br/>

- _Activation function for the output layer : Softmax Function_ <br/>

  - _The softmax function is commonly used as the activation function for the output layer in multi-class classification problems._ <br/>

  - _The softmax function normalizes the input values to calculate the probability of belonging to each class, and the sum of probabilities for all classes is 1._<br/><br/>

- _Optimization Algorithm : Adam (Adaptive Moment Estimation)_ <br/>

  - _The Adam optimization algorithm, which combines the advantages of Momentum, which adjusts the learning rate considering the direction of gradients, and RMSProp, which adjusts the learning rate considering the magnitude of gradients, was used._ <br/>

  - _The softmax function normalizes the input values to calculate the probability of belonging to each class, and the sum of probabilities for all classes is 1._<br/><br/>

- _Loss Function : Cross-Entropy Loss Function_ <br/>

  - _When using the softmax function in the output layer, the cross-entropy loss function is commonly used as the loss function._ <br/>

  - _The cross-entropy loss function calculates the error only for the classes corresponding to the actual target values and updates the model in the direction of minimizing the error._<br/><br/>

- _Evaluation Metric : Accuracy_ <br/>

  - _Accuracy is one of the evaluation metrics used to assess the performance of a classification model._ <br/>

  - _Accuracy considers the prediction as correct if it matches the actual target class and calculates it by dividing it by the total number of samples._<br/><br/>

- _Batch Size & Maximum Number of Learning Iterations_ <br/>

  - _In this experiment, the batch size is 128, and the model is trained by iterating up to a maximum of 100 times._<br/>
  
  - _The number of batch size and iterations during training affects the speed and accuracy of the model, and I, as the researcher conducting the experiment, have set the number of batch size and iterations based on my experience of tuning deep learning models._<br/><br/> <br/> 

### 3. &nbsp; Data Preprocessing and Analysis <br/><br/>

- _**Package Settings**_ <br/> 
  
  ```
  from keras import initializers
  from keras.utils import np_utils
  from keras.datasets import fashion_mnist

  from keras.optimizers import Adam
  from keras.models import Sequential
  from keras.layers import Flatten, Dense, Dropout, Input, Conv2D, MaxPooling2D
  from sklearn.model_selection import train_test_split


  import numpy as np
  import pandas as pd
  import seaborn as sns
  import matplotlib.pyplot as plt
  ```

- _**Data Preparation**_ <br/> 
  
  ```
  # í•™ìŠµìš©, ê²€ì¦ìš©, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ MNIST ë°ì´í„° ì…‹íŠ¸ ë¡œë”©
  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
  x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)

  # í•™ìŠµìš© & ê²€ì¦˜ìš© & í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì˜ ì°¨ì›
  print(f"í•™ìŠµìš© ë°ì´í„°ì˜ ì°¨ì› : ì…ë ¥ ë°ì´í„° {x_train.shape} / ë¼ë²¨ ë°ì´í„° / {y_train.shape}") 
  print(f"ê²€ì¦ìš© ë°ì´í„°ì˜ ì°¨ì› : ì…ë ¥ ë°ì´í„° {x_val.shape} / ë¼ë²¨ ë°ì´í„° / {y_val.shape}")
  print(f"í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì˜ ì°¨ì› : ì…ë ¥ ë°ì´í„° {x_test.shape} / ë¼ë²¨ ë°ì´í„° / {y_test.shape}")
  ```
  
  ```
  # 10ê°œì˜ ì´ë¯¸ì§€ì™€ ëª©í‘œ ë³€ìˆ˜ë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥
  plt.figure(figsize=(12, 2))
  for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(str(y_train[i]))
    plt.axis('off')
  plt.show()
  ```
  
  <img src="https://github.com/qortmdgh4141/Comparing-Performance-of-MLP-and-CNN-for-Classification-Problem/blob/main/image/image_label_graph.png?raw=true">
  
- _**Exploratory Data Analysis (EDA)**_ <br/> 
  
  ```
  # ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì› ë³€í™˜ : 3ì°¨ì›(ì´ë¯¸ì§€ ìˆ˜, 28, 28) -> 2ì°¨ì› (ì´ë¯¸ì§€ ìˆ˜, 784)
  x_train_reshaped = x_train.reshape(x_train.shape[0], 784)

  # ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ë„ ê°’ì˜ ë¹ˆë„ í™•ì¸
  x_train_df = pd.DataFrame(x_train_reshaped)
  total_null_count = x_train_df.isnull().sum().sum()
  print(f"ë„ê°’ì˜ ê°œìˆ˜ : {total_null_count}ê°œ")
  ```

  ```
  # ê° ì—´ë³„ë¡œ í”½ì…€ì˜ ê°•ë„ ë¶„ì„
  x_train_df.describe()
  ```
  
  ```
  # ëª©í‘œë³€ìˆ˜ì˜ ë¼ë²¨ë³„ ë¹ˆë„ ê³„ì‚° í›„ ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
  y_cnt = pd.DataFrame(y_train).value_counts()
  df = pd.DataFrame(y_cnt, columns=['Count'])

  # ì¸ë±ìŠ¤ ë¦¬ì…‹ ë° ë¬¸ìì—´ë¡œ ë³€í™˜
  df.reset_index(inplace=True)  
  df['Label'] = df[0].astype(str)

  # ì»¬ëŸ¬ë§µ ì„¤ì • ë° ë°”ì°¨íŠ¸ ìƒì„±
  cmap = plt.cm.Set3 
  fig, ax = plt.subplots(figsize=(12, 3)) 
  bars = ax.bar(df['Label'], df['Count'], color=cmap(np.arange(len(df))))

  # ë°” ìœ„ì— ë¼ë²¨ ê°¯ìˆ˜ ì¶œë ¥
  for i, count in enumerate(df['Count']):
      ax.text(i, count + 100, str(count), ha='center', fontsize=7)

  # ê·¸ë˜í”„ ë ˆì´ë¸”ê³¼ ì œëª© ì„¤ì • ë°  yì¶• ë²”ìœ„ ëŠ˜ë¦¬ê¸° (í˜„ì¬ ìµœëŒ“ê°’ì˜ 110%ë¡œ ë²”ìœ„ ì§€ì •)
  ax.set_xlabel('Label')
  ax.set_ylabel('Frequency')
  ax.set_title('Label Counts')
  ax.set_ylim(0, df['Count'].max() * 1.1)

  plt.show() # ê·¸ë˜í”„ ì¶œë ¥
  ```
  
  <img src="https://github.com/qortmdgh4141/Comparing-Performance-of-MLP-and-CNN-for-Classification-Problem/blob/main/image/vertical_bar_graph.png?raw=true">
    
- _**Feature Scaling**_ <br/>  
  
  ```
  # ì…ë ¥ë°ì´í„°ëŠ” ëª¨ë‘ 0~255 ì‚¬ì´ ê°’ì´ê¸° ë•Œë¬¸ì— ê°ê° 255ë¡œ ë‚˜ëˆ„ì–´ 0~1ë¡œ ì •ê·œí™”
  x_train = x_train.astype('float32') / 255
  x_val = x_val.astype('float32') / 255
  x_test = x_test.astype('float32') / 255  
  ```
  
- _**One-Hot Encoding**_ <br/> 
  
  ```
  # ë¼ë²¨ ë°ì´í„°ì˜ ì›-í•« ì¸ì½”ë”©
  y_train = np_utils.to_categorical(y_train)
  y_val = np_utils.to_categorical(y_val)
  y_test = np_utils.to_categorical(y_test)
  <br/> 

### 4. &nbsp; Training and Testing MLP Model <br/><br/>

- _Optimized MLP Model_

  ```
  """
  1. ì™„ì „ì—°ê²° ê³„ì¸µ (Dense Layer)
      - ë…¸ë“œ ìˆ˜: 512 or 10
      - ì™„ì „ì—°ê²° ê³„ì¸µì€ ëª¨ë“  ì…ë ¥ê³¼ ì¶œë ¥ì„ ì—°ê²°í•˜ëŠ” ì „í†µì ì¸ ì‹ ê²½ë§ ê³„ì¸µ
      - ì¶”ìƒì ì¸ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰

  2. ë“œë¡­ì•„ì›ƒ(Dropout) ì¸µ
      - ì‹ ê²½ë§ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì •ê·œí™” ê¸°ë²•ì¸ ë“œë¡­ì•„ì›ƒ(Dropout) ì¸µì„ ì¶”ê°€
      - ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ê³¼ì • ì¤‘ì— ì‹ ê²½ë§ì˜ ì¼ë¶€ ìœ ë‹›(neuron)ì„ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ë¹„í™œì„±í™”ì‹œí‚´ìœ¼ë¡œì¨,
        ëª¨ë¸ì´ íŠ¹ì • ìœ ë‹›ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê±° ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒ

  3. ì€ë‹‰ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ :  Relu
     - ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ê²½ìš°ëŠ” 0ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , 0ë³´ë‹¤ í° ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì¸ Relu í•¨ìˆ˜ë¡œ ì„¤ì •
     - ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” He ì´ˆê¹ƒê°’ì„ ì‚¬ìš©

  4. ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ :  Softmax
     - ì£¼ë¡œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì¸  Softmaxë¡œ ì„¤ì •
     - Softmax í•¨ìˆ˜ëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ì •ê·œí™”í•˜ì—¬ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•˜ë©°, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì˜ í•©ì€ 1

  5. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ : Adam
     - Momentumê³¼ RMSPropì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì¸ Adam(Adaptive Moment Estimation)ì„ ì‚¬ìš©
     - Momentumì€ : ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ 
     - RMSProp : ê¸°ìš¸ê¸° í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ

  6. ì†ì‹¤ í•¨ìˆ˜ : Cross-Entropy Loss Function
     - ì¶œë ¥ì¸µì—ì„œ Softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì†ì‹¤ í•¨ìˆ˜ë¡œëŠ” ì£¼ë¡œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©
     - í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜(Cross-Entropy Loss Function)ëŠ” ì‹¤ì œ íƒ€ê¹ƒ ê°’ì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì— ëŒ€í•´ì„œë§Œ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ë©°, 
       ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰

  7. ì •í™•ë„ í‰ê°€ ì§€í‘œ : Accuracy
     - ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ì¸ Accuracyë¥¼ ì‚¬ìš©
     - ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ê°€ ì‹¤ì œ íƒ€ê¹ƒ í´ë˜ìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë¥¼ ì •í™•í•œ ë¶„ë¥˜ë¡œ ê°„ì£¼í•˜ê³ , ì´ë¥¼ ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •í™•ë„ë¥¼ ê³„ì‚°

  8. ë°°ì¹˜ ì‚¬ì´ì¦ˆ / í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ / í•™ìŠµë¥  : 128 / 100 / 0.001
  """
  # ëª¨í˜• êµ¬ì¡°  
  mlp_model = Sequential()
  mlp_model.add(Flatten(input_shape=(28, 28)))
  mlp_model.add(Dropout(0.5))
  mlp_model.add(Dense(512, activation='relu', kernel_initializer=initializers.HeNormal()))
  mlp_model.add(Dense(10, activation='softmax'))

  mlp_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy']) 

  mlp_model.summary() # ëª¨í˜• êµ¬ì¡° ì¶œë ¥ 
  ```

  ```
  # í•™ìŠµ
  results_mlp = mlp_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=128)
  ```
    
  ```
  # í•™ìŠµëœ ëª¨í˜• í…ŒìŠ¤íŠ¸ 
  mlp_score = mlp_model.evaluate(x_test, y_test)
  mlp_accuracy = round(mlp_score[1]*100, 2)
  print(f"MLP ëª¨ë¸ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ : {round(mlp_score[0], 2)}")
  print(f"MLP ëª¨ë¸ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì •í™•ë„      : {mlp_accuracy}%")
  ```
  
  ```
  # í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡
  mlp_y_pred = mlp_model.predict(x_test)

  # ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ë¼ë²¨
  mlp_y_pred_class = np.argmax(mlp_y_pred, axis=1)
  y_test_class = np.argmax(y_test, axis=1)

  # êµì°¨í‘œ : ì‹¤ì œ ê°’ ëŒ€ë¹„ ì˜ˆì¸¡ ê°’ (ì£¼ëŒ€ê°ì›ì†Œì˜ ê°’ì´ ì •í™•í•˜ê²Œ ë¶„ë¥˜ëœ ë¹ˆë„, ê·¸ ì™¸ëŠ” ì˜¤ë¶„ë¥˜ ë¹ˆë„)
  mlp_crosstab = pd.crosstab(y_test_class,mlp_y_pred_class)
  mlp_crosstab
  ```
  <br/> 
  
### 4. &nbsp; Training and Testing CNN Model <br/><br/>

- _Optimized MLP Model_

  ```
  """
  1. í•©ì„±ê³± ì¸µ (Convolutional Layer)
      - í•„í„° ê°œìˆ˜: 32 or 64, ì»¤ë„ í¬ê¸° : (3, 3)
      - í•©ì„±ê³± ì¸µì€ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ í•„í„°(ì»¤ë„)ë¥¼ ì´ìš©í•˜ì—¬ ì§€ì—­ì ì¸ íŠ¹ì§•ì„ ì¶”ì¶œ íŠ¹ì„± ë§µ(Feature Map)ì„ ìƒì„±

  2. í’€ë§ ì¸µ (Pooling Layer) 
      - ìµœëŒ€ í’€ë§ í¬ê¸°: (2, 2)
      - í’€ë§ ì¸µì€ ê³µê°„ì ì¸ ë¶ˆë³€ì„±ì„ ì œê³µí•˜ê³ , íŠ¹ì„± ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ê³ , ì¶”ìƒí™”ëœ íŠ¹ì§•ì„ ë” ê°•ì¡°í•¨

  3. ì™„ì „ì—°ê²° ê³„ì¸µ (Dense Layer)
      - ë…¸ë“œ ìˆ˜: 512 or 10
      - ì™„ì „ì—°ê²° ê³„ì¸µì€ ëª¨ë“  ì…ë ¥ê³¼ ì¶œë ¥ì„ ì—°ê²°í•˜ëŠ” ì „í†µì ì¸ ì‹ ê²½ë§ ê³„ì¸µ
      - ì¶”ìƒì ì¸ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰

  4. ë“œë¡­ì•„ì›ƒ(Dropout) ì¸µ
      - ì‹ ê²½ë§ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì •ê·œí™” ê¸°ë²•ì¸ ë“œë¡­ì•„ì›ƒ(Dropout) ì¸µì„ ì¶”ê°€
      - ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ê³¼ì • ì¤‘ì— ì‹ ê²½ë§ì˜ ì¼ë¶€ ìœ ë‹›(neuron)ì„ ì„ì˜ë¡œ ì„ íƒí•˜ì—¬ ë¹„í™œì„±í™”ì‹œí‚´ìœ¼ë¡œì¨,
        ëª¨ë¸ì´ íŠ¹ì • ìœ ë‹›ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê±° ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒ

  5. ì€ë‹‰ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ :  Relu
     - ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ê²½ìš°ëŠ” 0ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , 0ë³´ë‹¤ í° ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì¸ Relu í•¨ìˆ˜ë¡œ ì„¤ì •
     - ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” He ì´ˆê¹ƒê°’ì„ ì‚¬ìš©

  6. ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ :  Softmax
     - ì£¼ë¡œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì¸  Softmaxë¡œ ì„¤ì •
     - Softmax í•¨ìˆ˜ëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ì •ê·œí™”í•˜ì—¬ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•˜ë©°, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì˜ í•©ì€ 1

  7. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ : Adam
     - Momentumê³¼ RMSPropì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì¸ Adam(Adaptive Moment Estimation)ì„ ì‚¬ìš©
     - Momentumì€ : ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ 
     - RMSProp : ê¸°ìš¸ê¸° í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ

  8. ì†ì‹¤ í•¨ìˆ˜ : Cross-Entropy Loss Function
     - ì¶œë ¥ì¸µì—ì„œ Softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì†ì‹¤ í•¨ìˆ˜ë¡œëŠ” ì£¼ë¡œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©
     - í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜(Cross-Entropy Loss Function)ëŠ” ì‹¤ì œ íƒ€ê¹ƒ ê°’ì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì— ëŒ€í•´ì„œë§Œ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ë©°, 
       ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰

  9. ì •í™•ë„ í‰ê°€ ì§€í‘œ : Accuracy
     - ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜ì¸ Accuracyë¥¼ ì‚¬ìš©
     - ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ê°€ ì‹¤ì œ íƒ€ê¹ƒ í´ë˜ìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë¥¼ ì •í™•í•œ ë¶„ë¥˜ë¡œ ê°„ì£¼í•˜ê³ , ì´ë¥¼ ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •í™•ë„ë¥¼ ê³„ì‚°

  10. ë°°ì¹˜ ì‚¬ì´ì¦ˆ / í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ / í•™ìŠµë¥  : 128 / 100 / 0.001

  # ëª¨í˜• êµ¬ì¡°
  cnn_model = Sequential()
  cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), kernel_initializer=initializers.HeNormal()))
  cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), kernel_initializer=initializers.HeNormal()))
  cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
  cnn_model.add(Dropout(0.5))

  cnn_model.add(Flatten())
  cnn_model.add(Dense(512, activation='relu', kernel_initializer=initializers.HeNormal()))
  cnn_model.add(Dropout(0.5))
  cnn_model.add(Dense(10, activation='softmax'))

  cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

  cnn_model.summary() # ëª¨í˜• êµ¬ì¡° ì¶œë ¥ 
  ```

  ```
  # í•™ìŠµ
  results_cnn = cnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=128)
  ```
    
  ```
  # í•™ìŠµëœ ëª¨í˜• í…ŒìŠ¤íŠ¸ 
  cnn_score = cnn_model.evaluate(x_test, y_test)
  cnn_accuracy = round(cnn_score[1]*100, 2)
  print(f"CNN ëª¨ë¸ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ : {round(cnn_score[0], 2)}")
  print(f"CNN ëª¨ë¸ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì •í™•ë„      : {cnn_accuracy}%")
  ```
  
  ```
  # í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ˆì¸¡
  cnn_y_pred = cnn_model.predict(x_test)

  # ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ë¼ë²¨
  cnn_y_pred_class = np.argmax(cnn_y_pred, axis=1)
  y_test_class = np.argmax(y_test, axis=1)

  # êµì°¨í‘œ : ì‹¤ì œ ê°’ ëŒ€ë¹„ ì˜ˆì¸¡ ê°’ (ì£¼ëŒ€ê°ì›ì†Œì˜ ê°’ì´ ì •í™•í•˜ê²Œ ë¶„ë¥˜ëœ ë¹ˆë„, ê·¸ ì™¸ëŠ” ì˜¤ë¶„ë¥˜ ë¹ˆë„)
  cnn_crosstab = pd.crosstab(y_test_class, cnn_y_pred_class)
  cnn_crosstab
  ```
  <br/> 

### 5. &nbsp; Research Results  <br/><br/>
    
- _The objective of this study was to compare the classification performance between the Original-ResNet50 model and the TL-ResNet-50 model with transfer learning (TL) technique applied, using the AI-Hub Korean face image dataset for estimating person's age._ <br/> <br/> 
  
  ```
  def plot_loss_and_accuracy(train_loss, val_loss, train_acc, val_acc, model_name):
      epochs = range(1, len(train_loss) + 1)

      plt.figure(figsize=(12, 6))

      # Loss ê·¸ë˜í”„
      plt.subplot(2, 2, 1)
      plt.plot(epochs, train_loss, 'b', label='Training Loss')
      plt.plot(epochs, val_loss, 'r', label='Validation Loss')
      plt.title(f'{model_name} Model - Training and Validation Loss')
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.legend()

      # Accuracy ê·¸ë˜í”„
      plt.subplot(2, 2, 2)
      plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
      plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
      plt.title(f'{model_name} Model - Training and Validation Accuracy')
      plt.xlabel('Epoch')
      plt.ylabel('Accuracy')
      plt.legend()

      plt.tight_layout()

      plt.show()

  # MLP ëª¨ë¸ ê²°ê³¼ ê·¸ë˜í”„ ì¶œë ¥
  plot_loss_and_accuracy(results_mlp.history['loss'], results_mlp.history['val_loss'],
                         results_mlp.history['accuracy'], results_mlp.history['val_accuracy'], 'MLP')

  # CNN ëª¨ë¸ ê²°ê³¼ ê·¸ë˜í”„ ì¶œë ¥
  plot_loss_and_accuracy(results_cnn.history['loss'], results_cnn.history['val_loss'],
                         results_cnn.history['accuracy'], results_cnn.history['val_accuracy'], 'CNN')
  ```
  
  <img src="https://github.com/qortmdgh4141/Comparing-Performance-of-MLP-and-CNN-for-Classification-Problem/blob/main/image/line_graph.png?raw=true">

- _Analyzing the "Training Loss" graph, we observe that the Original-ResNet50 model experiences underfitting from the 2nd epoch onwards, with the loss value not decreasing gradually. In contrast, the TF-ResNet50 model consistently exhibits a gradual decrease in the loss value even after the 2nd epoch. Moreover, starting from the 1st epoch, the TF-ResNet50 model, which utilizes the pre-trained model's weights as initial values, outputs significantly lower loss values compared to the Original-ResNet50 model._ <br/>
 
- _The superiority of the TF-ResNet model is also evident in the "Accuracy on Validation Data" graph, where the TF-ResNet50 model consistently achieves considerably higher accuracy values than the Original-ResNet50 model across all epochs. Notably, there is a substantial difference in accuracy values between the two models, particularly when reaching the final 10th epoch._ <br/><br/> 

  ```
  def gradientbars(bars, cmap_list):
      grad = np.atleast_2d(np.linspace(0, 1, 256)).T
      ax = bars[0].axes
      lim = ax.get_xlim() + ax.get_ylim()
      ax.axis(lim)
      max_width = max([bar.get_width() for bar in bars])
      for i, bar in enumerate(bars):
          bar.set_facecolor("none")
          x, y = bar.get_xy()
          w, h = bar.get_width(), bar.get_height()
          ax.imshow(grad, extent=[x, x + w, y, y + h], aspect="auto", cmap=cmap_list[i])
          plt.text(w + 0.7, y + h / 2.0 + 0.015, "{}".format(int(w)), fontsize=8, ha='left', va='center')

  # MLP ëª¨ë¸ ë° CNN ëª¨ë¸ì˜ ì˜¤ë¶„ë¥˜ ë¹ˆë„
  mlp_error_count = len(y_test_class) - np.sum(y_test_class == mlp_y_pred_class)
  cnn_error_count = len(y_test_class) - np.sum(y_test_class == cnn_y_pred_class)
  error_counts = [mlp_error_count, cnn_error_count]

  # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì˜¤ë¶„ë¥˜ ë¹ˆë„ í‘œí˜„
  models = ['MLP', 'CNN']
  cmap_list = ['Reds', 'Blues']

  fig, ax = plt.subplots(figsize=(12, 4))
  bars = ax.barh(models, error_counts, color='white', alpha=0.7)
  gradientbars(bars, cmap_list)

  ax.set_ylabel('Model', fontsize=12)
  ax.set_xlabel('Error Count', fontsize=12)
  ax.set_title('< Error Count Comparison between MLP and CNN >', fontsize=10)

  plt.show()

  ```
  
  <img src="https://github.com/qortmdgh4141/Comparing-Performance-of-MLP-and-CNN-for-Classification-Problem/blob/main/image/horizontal_bar_graph.png?raw=true">
  
- _Finally, I compared the accuracy values of the two models on real test data and found that the TF-ResNet model has about 2x higher accuracy than the Original-ResNet50 model. These results prove that models using transfer learning perform better when training with small datasets and a limited number of epochs._ <br/> <br/> <br/>
 
--------------------------
### ğŸ’» S/W Development Environment
<p>
  <img src="https://img.shields.io/badge/Windows 10-0078D6?style=flat-square&logo=Windows&logoColor=white"/>
  <img src="https://img.shields.io/badge/Google Colab-black?style=flat-square&logo=Google Colab&logoColor=yellow"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
</p>
<p>
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white"/>
  <img src="https://img.shields.io/badge/scikit learn-blue?style=flat-square&logo=scikitlearn&logoColor=F7931E"/>
  <img src="https://img.shields.io/badge/Numpy-013243?style=flat-square&logo=Numpy&logoColor=blue"/>
</p>

### ğŸš€ Machine Learning Model
<p>
  <img src="https://img.shields.io/badge/MLP-5C5543?style=flat-square?"/>
  <img src="https://img.shields.io/badge/CNN-4169E1?style=flat-square?"/>
</p> 

### ğŸ’¾ Datasets used in the project
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fashion-MNIST Dataset <br/>
